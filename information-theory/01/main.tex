\documentclass[journal,12pt,twocolumn]{IEEEtran}
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{xcolor}
\usepackage{caption}
\singlespacing
\usepackage{siunitx}
\usepackage[cmex10]{amsmath}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{txfonts}
\usepackage{stfloats}
\usepackage{cite}
\usepackage{cases}
\usepackage{subfig}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage{circuitikz}
\let\vec\mathbf
\DeclareMathOperator*{\Res}{Res}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}
\hyphenation{op-tical net-works semi-conduc-tor}

\lstset{
language=Python,
frame=single, 
breaklines=true,
columns=fullflexible
}
\begin{document}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\newcommand{\myvec}[1]{\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}
\newcommand{\myaugvec}[2]{\ensuremath{\begin{amatrix}{#1}#2\end{amatrix}}}
\newcommand{\mydet}[1]{\ensuremath{\begin{vmatrix}#1\end{vmatrix}}}
\bibliographystyle{IEEEtran}
\providecommand{\nCr}[2]{\,^{#1}C_{#2}} % nCr
\providecommand{\nPr}[2]{\,^{#1}P_{#2}} % nPr
\providecommand{\mbf}{\mathbf}
\providecommand{\pr}[1]{\ensuremath{\Pr\left(#1\right)}}
\providecommand{\qfunc}[1]{\ensuremath{Q\left(#1\right)}}
\providecommand{\sbrak}[1]{\ensuremath{{}\left[#1\right]}}
\providecommand{\lsbrak}[1]{\ensuremath{{}\left[#1\right.}}
\providecommand{\rsbrak}[1]{\ensuremath{{}\left.#1\right]}}
\providecommand{\brak}[1]{\ensuremath{\left(#1\right)}}
\providecommand{\lbrak}[1]{\ensuremath{\left(#1\right.}}
\providecommand{\rbrak}[1]{\ensuremath{\left.#1\right)}}
\providecommand{\cbrak}[1]{\ensuremath{\left\{#1\right\}}}
\providecommand{\lcbrak}[1]{\ensuremath{\left\{#1\right.}}
\providecommand{\rcbrak}[1]{\ensuremath{\left.#1\right\}}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}
\newcommand{\rect}{\mathop{\mathrm{rect}}}
\newcommand{\sinc}{\mathop{\mathrm{sinc}}}
\providecommand{\abs}[1]{\left\vert#1\right\vert}
\providecommand{\res}[1]{\Res\displaylimits_{#1}} 
\providecommand{\norm}[1]{\left\Vert#1\right\Vert}
\providecommand{\mtx}[1]{\mathbf{#1}}
\providecommand{\mean}[1]{E\left[ #1 \right]}
\providecommand{\fourier}{\overset{\mathcal{F}}{ \rightleftharpoons}}
\providecommand{\ztrans}{\overset{\mathcal{Z}}{ \rightleftharpoons}}
\providecommand{\system}[1]{\overset{\mathcal{#1}}{ \longleftrightarrow}}
\newcommand{\solution}{\noindent \textbf{Solution: }}
\providecommand{\dec}[2]{\ensuremath{\overset{#1}{\underset{#2}{\gtrless}}}}
\let\StandardTheFigure\thefigure
\def\putbox#1#2#3{\makebox[0in][l]{\makebox[#1][l]{}\raisebox{\baselineskip}[0in][0in]{\raisebox{#2}[0in][0in]{#3}}}}
     \def\rightbox#1{\makebox[0in][r]{#1}}
     \def\centbox#1{\makebox[0in]{#1}}
     \def\topbox#1{\raisebox{-\baselineskip}[0in][0in]{#1}}
     \def\midbox#1{\raisebox{-0.5\baselineskip}[0in][0in]{#1}}
\numberwithin{equation}{section}

\vspace{3cm}
\title{Chapter 1: Source Coding}
\author{Gautam Singh}
\maketitle
\bigskip

\section{Conventions}

\begin{enumerate}[label=\thesection.\arabic*, ref=\thesection.\theenumi]
    \item $X$ denotes a random variable.
    \item $\mathcal{X}$ denotes the alphabet.
    \item $x$ denotes a particular value of the alphabet.
    \item $p(x) \define \pr{X = x}$.
\end{enumerate}

\section{Uncertainity and Information}

\begin{enumerate}[label=\thesection.\arabic*, ref=\thesection.\theenumi]
    \item \textbf{Self Information:} The self information of the event $X = x$ 
    is defined as
    \begin{align}
        I(x) \define \log\brak{\frac{1}{\log{p\brak{x}}}} = -\log\brak{p(x)}
        \label{eq:self-info}
    \end{align}
    Clearly, $I(x) = 0$ at $p(x) = 1$, that is, a high probability event conveys
    lesser information.
    \item The units are determined by the base of the algorithm
    \begin{enumerate}[label=\theenumi.\arabic*, ref=\theenumi.\arabic*]
        \item If the base is 2, the units are \textbf{bits}.
        \item If the base is $e$, the units are \textbf{nats}.
        \item If the base is 10, the units are \textbf{dits}.
    \end{enumerate}
    \item \textbf{Mutual Information:} The mutual information between $x$ and $y$ 
    is defined as
    \begin{align}
        I(x;y) \define \log\brak{\frac{p(x|y)}{p(x)}}
        \label{eq:mutual-info}
    \end{align}
    Observe that
    \begin{align}
        I(x;y) &= \log\brak{\frac{p(x|y)}{p(x)}} \\
        &= \log\brak{\frac{p(x|y)p(y)}{p(x)p(y)}} \\
        &= \log\brak{\frac{p(x,y)}{p(x)p(y)}} \\
        &= \log\brak{\frac{p(y|x)}{p(y)}} = I(y;x)
        \label{eq:mutual-info-order}
    \end{align}
    \item Can be interpreted as the information event $Y = y$ provides about
    $X = x$. 
    \item Equation \eqref{eq:mutual-info-order} can be interpreted as follows
    \begin{quote}
    The amount of information about $X = x$ provided by $Y = y$ is the same as 
    the amount of information about $Y = y$ provided by $X = x$.
    \end{quote}
    \item Notice that when $X$ and $Y$ are independent, then $I(x;y) = 0$ as 
    $p(x,y) = p(x)p(y)$. Similarly, if $p(x|y) = 1$, then $I(x;y) = I(x)$.
    \item \textbf{Conditional Self Information:} The conditional self information 
    of the event $X = x$ given $Y = y$ is defined as
    \begin{align}
        I(x|y) \define \log\brak{\frac{1}{p(x|y)}} = -\log p(x|y)
        \label{eq:conditional-self-info}
    \end{align}
    Notice that
    \begin{align}
        I(x;y) = \log\brak{\frac{p(x|y)}{p(x)}} = I(x) - I(x|y)
        \label{eq:cond-mutual-self-info}
    \end{align}
    And thus mutual information can be positive, negative or zero.
\end{enumerate}

\section{Average Mutual Information and Entropy}

\begin{enumerate}[label=\thesection.\arabic*, ref=\thesection.\theenumi]
    \item \textbf{Average Mutual Information:} The average mutual information
    between random variables $X$ and $Y$ is defined as
    \begin{align}
        I(X;Y) &\define \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)T(x;y) \label{eq:avg-mutual-info-1} \\
        &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log\brak{\frac{p(x,y)}{p(x)p(y)}} \label{eq:avg-mutual-info-2} \\
        &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x)p(y|x)\log\brak{\frac{p(y|x)}{p(x)}} \label{eq:avg-mutual-info-3} \\
        &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(y)p(x|y)\log\brak{\frac{p(x|y)}{p(y)}} \label{eq:avg-mutual-info-4} \\
        &= E\sbrak{\log\brak{\frac{p(X,Y)}{p(X)p(Y)}}} \label{eq:avg-mutual-info-5} \\
        &= E\sbrak{-\log\brak{\frac{p(X)p(Y)}{p(X,Y)}}} \label{eq:avg-mutual-info-6}
    \end{align}
    \item When $X$ and $Y$ are independent, \eqref{eq:avg-mutual-info-5} gives 
    $I(X;Y) = 0$, that is, there is no average information between $X$ and $Y$.
    \item In general, $I(X;Y) \ge 0$ with equality iff $X$ and $Y$ are independent.
    \item \textbf{Average Self Information/Entropy:} The average self information 
    or entropy of a random variable $X$ is defined as
    \begin{align}
        H(X) &\define \sum_{x\in\mathcal{X}}p(x)I(x) \label{eq:entropy-1} \\
        &= \sum_{x\in\mathcal{X}}p(x)\log\brak{\frac{1}{p(x)}} \label{eq:entropy-2} \\
        &= E\sbrak{\log\brak{\frac{1}{p(X)}}} \label{eq:entropy-3} \\
        &= E\sbrak{-\log p(X)} \label{eq:entropy-4}
    \end{align}
    \item Notice that since $0 \le p(x) \le 1$, \eqref{eq:entropy-1} gives 
    $H(X) \ge 0$.
    \item The units of $I(X;Y)$ and $H(X)$ are \textbf{bits}.
    \item For a Bernoulli trial with success rate $p$, the entropy of the
    outcome $X$ is
    \begin{align}
        H(X) &= -\brak{p\log_2p + (1 - p)\log_2\brak{1 - p}}
        \label{eq:bin-entropy-func}
    \end{align}
    which is known as the \textit{binary entropy function} and denoted by 
    $h_2(p)$.
    \item \textbf{Average Conditional Self Information/Conditional Entropy:}
    The average self information or conditional entropy of a random variable 
    $X$ given a random variable $Y$ is defined as
    \begin{align}
        H(X|Y) &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log\frac{1}{p(x|y)} \label{eq:cond-entropy-1} \\
        &= E\sbrak{\log\frac{1}{p(X|Y)}} \label{eq:cond-entropy-2} \\
        &= E\sbrak{-\log p(X|Y)} \label{eq:cond-entropy-3}
    \end{align}
    Clearly,
    \begin{align}
        I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)
        \label{eq:cond-mutual-self-entropy}
    \end{align}
    \item Note that $I(X;Y) \ge 0 \implies H(X) \ge H(X|Y)$. Thus, conditioning 
    can only decrease entropy. In case it does not, $X$ and $Y$ are 
    independent.
    \item \textbf{Joint Entropy:} The joint entropy of a pair of discrete random
    variables $(X,Y)$ with a joint pmf $p(x,y)$ is defined as
    \begin{align}
        H(X,Y) &\define \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log\frac{1}{p(x,y)} \label{eq:joint-entropy-1} \\
        &= -\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log{p(x,y)} \label{eq:joint-entropy-2} \\
        &= E\sbrak{\log\frac{1}{p(X,Y)}} \label{eq:joint-entropy-3} \\
        &= E\sbrak{-\log{p(X,Y)}} \label{eq:joint-entropy-4}
    \end{align}
    In general, the joint entropy of an $n$-tuple of random variables 
    $\brak{X_1, X_2, \ldots, X_n}$ with joint pmf $p\brak{X_1,X_2,\ldots,X_n}$ 
    is 
    \begin{align}
        H(X_1,X_2,\ldots,X_n) \define E\sbrak{-\log{p\brak{X_1,X_2,\ldots,X_n}}}
        \label{eq:joint-entropy-gen}
    \end{align}
    \item From \eqref{eq:entropy-4} and \eqref{eq:cond-entropy-3}, we get the 
    \textbf{chain rule}
    \begin{align}
        H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y) \label{eq:chain-rule-two}
    \end{align}
    In general for $n$ random variables $X_i,\ 1 \le i \le n$, the chain rule is 
    \begin{align}
        H\brak{X_1,X_2,\ldots,X_n} = \sum_{i=1}^n H\brak{X_i|X_1,X_2,\ldots,X_{i-1}}
        \label{eq:chain-rule-gen}
    \end{align}
    \item From \eqref{eq:avg-mutual-info-6}, we clearly see
    \begin{align}
        I(X;Y) = H(X) + H(Y) - H(X,Y) \label{eq:mutual-self-eqn}
    \end{align}
\end{enumerate}

\section{Information Measures for Continuous Random Variables}

\begin{enumerate}[label=\thesection.\arabic*, ref=\thesection.\theenumi]
    \item \textbf{Average Mutual Information:} The average mutual information
    between two continuous random variables $X$ and $Y$ with joint pdf $p(x,y)$
    and marginal pdfs $p(x)$ and $p(y)$ respectively is defined as
    \begin{align}
        I(X;Y) \define \int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}p(x)p(y|x)\log\frac{p(y|x)}{p(y)}\ dxdy
        \label{eq:mutual-info-cont}
    \end{align}
    \item Note that while physical interpretation of mutual information can be
    applied here, such physical interpretations will not work with other 
    quantities. This is because the information in a continuous random variable
    is infinite. Hence, differential entropy is defined.
    \item \textbf{Differential Entropy:} The differential entropy of a continuous
    random variable $X$ is defined as
    \begin{align}
        h(X) \define -\int\limits_{-\infty}^{\infty}p(x)\log p(x)\ dx
        \label{eq:diff-entropy}
    \end{align}
    While there is no physical meaning for this quantity, the units remain bits.
    \item \textbf{Average Conditional Entropy:} The average conditional entropy 
    of a continuous random variable $X$ given $Y$ is defined as 
    \begin{align}
        h(X|Y) \define -\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}p(x,y)\log p(x|y)\ dxdy
        \label{eq:avg-cond-entropy-cont}
    \end{align}
    We can express the \textit{average mutual information} as 
    \begin{align}
        I(X;Y) = h(X) - h(X|Y) = h(Y) - h(Y|X)
        \label{eq:mutual-self-cond-cont-eqn}
    \end{align}
    \item Rules for differential entropy:
    \begin{enumerate}[label=\theenumi.\arabic*, ref=\theenumi.\arabic*]
        \item The chain rule for differential entropy is given as 
        \begin{align}
            h(X_1,X_2,\ldots,X_n) = \sum_{i=1}^nh(X_i|X_1,X_2,\ldots,X_{i-1})
            \label{eq:chain-rule-cont}
        \end{align}
        \item $h(X + c) = h(X)$, that is, transition does not alter differential
        entropy.
        \item $h(aX) = h(X) + \log\abs{a}$.
        \item If $X$ and $Y$ are independent, then we have $h(X+Y) \ge h(X+Y|Y) 
        = h(X|Y) = h(X)$.
    \end{enumerate}
\end{enumerate}

\section{Relative Entropy}
\begin{enumerate}[label=\thesection.\arabic*, ref=\thesection.\theenumi]
    \item \textbf{Relative Entropy or Kullback Leibler (KL) Distance:} The 
    relative entropy of Kullback Leibler Distance between two pmfs $p(x)$ 
    and $q(x)$ is defined as 
    \begin{align}
        D\brak{p\|q} &= \sum_{x\in\mathcal{X}}p(x)\log\brak{\frac{p(x)}{q(x)}} \label{eq:dkl-1} \\
        &= E\sbrak{\log\brak{\frac{p(X)}{q(X)}}} \label{eq:dkl-2}
    \end{align}
    It is sometimes denoted by $D_{KL}\brak{p\|q}$.
    \item We can rewrite the mutual information in terms of relative entropy 
    \begin{align}
        I(X;Y) = D(p(x,y)\|p(x)p(y)) \label{eq:mutual-info-dkl}
    \end{align}
    \item \textbf{Jensen Shannon Distance:} The Jensen Shannon distance between 
    two pmfs $p(x)$ and $q(x)$ is defined as 
    \begin{align}
        JSD\brak{p\|q} \define \frac{1}{2}\brak{D\brak{p\|m} + D\brak{q\|m}}
        \label{eq:jsd}
    \end{align}
    It is sometimes denoted by $D_{JS}\brak{p\|q}$, and referred to as 
    \textbf{Jensen Shannon Divergence} or \textbf{Information Radius}.
    \item \textbf{Convex Function:} A function $f$ defined on $\sbrak{0,1}$ is 
    said to be convex if
    \begin{align}
        f\brak{\lambda x_1 + \brak{1 - \lambda}x_2} \le \lambda f\brak{x_1} + \brak{1-\lambda}f\brak{x_2}
        \label{eq:convex}
    \end{align}
    for all $\lambda \in \sbrak{0,1}$. On the other hand, if $f$ is \textbf{concave}, 
    then the inequality in \eqref{eq:convex} becomes $\ge$. Strict inequalities
    would make $f$ \textbf{strictly convex} or \textbf{strictly concave}.
    \item $D(p\|q)$ is convex in the pair $\brak{p,q}$ and $H(p)$ is concave in $p$.
\end{enumerate}

\end{document}